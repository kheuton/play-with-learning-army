{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# add src to path\n",
    "import sys\n",
    "sys.path.append('/cluster/home/kheuto01/code/play-with-learning-army/src')\n",
    "import numpy as np\n",
    "# change directory to this files directory\n",
    "import os\n",
    "os.chdir('/cluster/home/kheuto01/code/play-with-learning-army')\n",
    "from data_loader import load_processed, make_dataset\n",
    "from embedder_registry import initialize_embedding, initialize_criteria_embedding, initialize_combiner\n",
    "from domain_models import initialize_domain_models\n",
    "from loss_opt import initialize_loss, initialize_optimizer\n",
    "import torch\n",
    "import yaml\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_directory = '/cluster/tufts/hugheslab/kheuto01/sensemaking/bertfinetune_test/test15_lr5e-07_alpha0.001_beta0.1'\n",
    "problem_config_path = '/cluster/home/kheuto01/code/play-with-learning-army/config/problem_config.yaml'\n",
    "test_metrics_path = os.path.join(experiment_directory, 'test_metrics.csv')\n",
    "config_path = os.path.join(experiment_directory, 'config.yaml')\n",
    "model_path = os.path.join(experiment_directory, 'final_model.pth')\n",
    "hyper_config = yaml.load(open(config_path, 'r'), Loader=yaml.FullLoader)\n",
    "problem_config = yaml.load(open(problem_config_path, 'r'), Loader=yaml.FullLoader)\n",
    "num_domains = problem_config['num_domains']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dict = torch.load(model_path, weights_only=False, map_location=torch.device('cpu'))\n",
    "domain_model_dict = save_dict['domain_model_dict']\n",
    "embedder = save_dict['embed_func']\n",
    "embedder.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_test_features, processed_test_labels = load_processed(hyper_config['test_x_file'], \n",
    "                                                                hyper_config['test_y_file'])\n",
    "(xs, ys, problem_ids, student_ids) = make_dataset(processed_test_features, processed_test_labels)\n",
    "xs, ys, problem_ids, student_ids = embedder.preprocess_data((xs, ys, torch.tensor(problem_ids), torch.tensor(student_ids)), hyper_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds, all_labels, all_weights, all_domains = [], [], [], []\n",
    "\n",
    "if isinstance(xs, transformers.BatchEncoding):\n",
    "    batch_length = len(next(iter(xs.values())))\n",
    "else:\n",
    "    batch_length = len(xs)\n",
    "    \n",
    "for i in range(batch_length):\n",
    "    if isinstance(xs, transformers.BatchEncoding):\n",
    "        x = {k: v[i].unsqueeze(0) for k, v in xs.items()}\n",
    "    else:\n",
    "        x = xs[i].unsqueeze(0)\n",
    "    y = ys[i]\n",
    "    p = problem_ids[i]\n",
    "    s = student_ids[i]\n",
    "\n",
    "    x_embed = embedder.forward(x)\n",
    "\n",
    "    criteria_counter = 0\n",
    "    for d in range(num_domains):\n",
    "        num_criteria = problem_config['problems'][p]['domains'][d][\"num_criteria\"]\n",
    "        for c in range(num_criteria):\n",
    "            c_embed = torch.tensor([c])\n",
    "            final_representation = torch.cat((x_embed, c_embed.unsqueeze(0)), dim=1)\n",
    "            y_pred = domain_model_dict[d](final_representation)\n",
    "            weight = 1/num_criteria\n",
    "\n",
    "            all_preds.append(y_pred.detach().cpu().numpy())\n",
    "            all_labels.append(y[criteria_counter].cpu().numpy())\n",
    "            all_weights.append(weight)\n",
    "            all_domains.append(d)\n",
    "            criteria_counter += 1\n",
    "\n",
    "all_preds_np = np.concatenate(all_preds).flatten()\n",
    "all_labels_np = np.array(all_labels).flatten()\n",
    "all_labels = torch.tensor(np.array(all_labels)).squeeze()\n",
    "all_preds = torch.tensor(all_preds).squeeze()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_metric_across_domains(y_pred, y_true, metric_func, domain_ids):\n",
    "    domain_metrics = {}\n",
    "    unique_domains = np.unique(domain_ids)\n",
    "    print(unique_domains)\n",
    "    for d in unique_domains:\n",
    "        d_idx = np.where(domain_ids == d)[0]\n",
    "        domain_metrics[d] = {}\n",
    "        _,_,_,domain_metrics[d]['bootstrap_scores'] = bootstrap_metric(y_pred[d_idx], y_true[d_idx], metric_func)\n",
    "        domain_metrics[d]['mean'] = domain_metrics[d]['bootstrap_scores'].mean()\n",
    "        domain_metrics[d]['std_err'] =domain_metrics[d]['bootstrap_scores'].std()/np.sqrt(len(domain_metrics[d]['bootstrap_scores']))\n",
    "    return domain_metrics\n",
    "\n",
    "#https://github.com/tufts-ml/SupContrast/blob/tmlr/bootstrap_lin_acc.py\n",
    "def bootstrap_metric(y_pred, y_true, metric_func,\n",
    "                     n_bootstraps=1000, rng_seed=123):\n",
    "    \"\"\"Compute test set boostrapping of a metric\n",
    "    Args:\n",
    "        y_pred (tensor): Model predictions for some output y\n",
    "        y_true (tensor): True value of output y\n",
    "        metric_func (function): function with parameters (y_pred, y_true)\n",
    "                                returning a Tensor castable metric\n",
    "        n_bootstraps (int, optional): Number of bootstrap samples to take.\n",
    "                                      Defaults to 200.\n",
    "        rng_seed (int, optional): Random seed for reproducibility.\n",
    "                                  Defaults to 123.\n",
    "    Returns:\n",
    "        tuple: metric_mean: Tensor with bootstrapped mean of metric\n",
    "               ci_low: Low value from 95% confidence interval\n",
    "               ci_high: High value from 95% confidence interval\n",
    "               b_scores: Bootstrapped metric outputs\n",
    "    \"\"\"\n",
    "    b_scores = None\n",
    "    rng = torch.random.manual_seed(rng_seed)\n",
    "    # bootstrap\n",
    "    for _ in range(n_bootstraps):\n",
    "        sample_idx = torch.randint(y_pred.shape[0], size=(y_pred.shape[0],), generator=rng)\n",
    "        score = torch.Tensor([metric_func(y_pred[sample_idx], y_true[sample_idx])])\n",
    "        # store results from each run along axis 0, with other axes' shape determined by metric\n",
    "        if b_scores is None:\n",
    "            b_scores = score.unsqueeze(0)\n",
    "        else:\n",
    "            b_scores = torch.vstack((b_scores, score))\n",
    "    # compute mean and confidence interval\n",
    "    metric_mean = torch.mean(b_scores, dim=0)\n",
    "    ci_low = torch.quantile(b_scores, 0.025, dim=0)\n",
    "    ci_high = torch.quantile(b_scores, 0.975, dim=0)\n",
    "    return (metric_mean, ci_low, ci_high, b_scores)\n",
    "\n",
    "\n",
    "def bootstrap_dif(b_scores_1, b_scores_2):\n",
    "    \"\"\"Examine the difference of two bootstrapped metrics\n",
    "\n",
    "    Args:\n",
    "        b_scores_1 (Tensor): Bootstrapped metric outputs, with same seed as 2\n",
    "        b_scores_2 (Tensor): Bootstrapped metric outputs, with same seed as 1\n",
    "    Returns:\n",
    "        tensor: True if 95% CI does not contain 0 so result is statistically significant\n",
    "                False if 95% CI contains 0 so result is not statistically significant\n",
    "    \"\"\"\n",
    "    dif_scores = b_scores_1 - b_scores_2\n",
    "    # compute confidence interval of the difference\n",
    "    ci_low = torch.quantile(dif_scores, 0.025, dim=0)\n",
    "    ci_high = torch.quantile(dif_scores, 0.975, dim=0)\n",
    "    return ~torch.logical_and(ci_low <= 0, ci_high >= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "/cluster/tufts/hugheslab/kheuto01/mambaforge/envs/learningarmy/lib/python3.11/site-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "domain_auprc = bootstrap_metric_across_domains(all_labels, all_preds, average_precision_score, all_domains)\n",
    "micro_auprc, _, _, micro_auprc_scores = bootstrap_metric(all_labels, all_preds, average_precision_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain Objects\n",
      "AUPRC: 0.982384204864502 +/- 0.0003669500001706183\n",
      "Properties: {'name': 'Objects', 'num_criteria': 1}\n",
      "\n",
      "\n",
      "Domain Influences\n",
      "AUPRC: 0.832855761051178 +/- 0.002788752783089876\n",
      "Properties: {'name': 'Influences', 'num_criteria': 0}\n",
      "\n",
      "\n",
      "Domain Properties\n",
      "AUPRC: 0.7330579161643982 +/- 0.00782721396535635\n",
      "Properties: {'name': 'Properties', 'num_criteria': 0}\n",
      "\n",
      "\n",
      "Domain Positioning\n",
      "AUPRC: 0.7180802226066589 +/- 0.0022123451344668865\n",
      "Properties: {'name': 'Positioning', 'num_criteria': 1}\n",
      "\n",
      "\n",
      "Domain Movements\n",
      "AUPRC: 0.751388669013977 +/- 0.0019306187750771642\n",
      "Properties: {'name': 'Movements', 'num_criteria': 3}\n",
      "\n",
      "\n",
      "Domain Interactions\n",
      "AUPRC: 0.8882994651794434 +/- 0.0014369270065799356\n",
      "Properties: {'name': 'Interactions', 'num_criteria': 0}\n",
      "\n",
      "\n",
      "Domain Descriptive Relationships\n",
      "AUPRC: 0.6425483822822571 +/- 0.0036470675840973854\n",
      "Properties: {'name': 'Descriptive Relationships', 'num_criteria': 2}\n",
      "\n",
      "\n",
      "Domain Mechanistic Relationships\n",
      "AUPRC: 0.647122323513031 +/- 0.004285923205316067\n",
      "Properties: {'name': 'Mechanistic Relationships', 'num_criteria': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_domain_properties = problem_config['problems'][0]['domains']\n",
    "for d in range(num_domains):\n",
    "    print(f\"Domain {all_domain_properties[d]['name']}\")\n",
    "    print(f\"AUPRC: {domain_auprc[d]['mean']} +/- {domain_auprc[d]['std_err']}\")\n",
    "    print(f\"Properties: {all_domain_properties[d]}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "micro_auprc: tensor([0.8618]) +/- 0.0006026943447068334\n"
     ]
    }
   ],
   "source": [
    "print(f'micro_auprc: {micro_auprc} +/- {micro_auprc_scores.std()/np.sqrt(len(micro_auprc_scores))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learningarmy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
